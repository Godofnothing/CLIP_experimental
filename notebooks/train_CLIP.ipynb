{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Define seed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 42"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Godofnothing/CLIP_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch-lightning\n",
    "!pip install -q ftfy regex\n",
    "!wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"CLIP_experimental\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src import CLIP_Lite, CLIP_Pro\n",
    "from src import TextTransformer\n",
    "from src import SimpleTokenizer\n",
    "from src import CLIPDataset, train_val_split\n",
    "from src import ClassificationVisualizer\n",
    "\n",
    "from srgan import GANUpsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "assert device == 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lite or pro\n",
    "clip_mode = 'lite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models available\n",
    "- RN50\n",
    "- RN101\n",
    "- RN50x4\n",
    "- ViT-B/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/openai/CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP import clip\n",
    "\n",
    "clip_backbone = \"ViT-B/32\"\n",
    "\n",
    "# set jit=False to case to nn.Module from torchscript \n",
    "model, image_transform = clip.load(clip_backbone, jit=False)\n",
    "input_resolution = 224\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bicubic or GAN upsample\n",
    "upsample_mode = 'bicubic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if upsample_mode == 'bicubic':\n",
    "    # download the origingal dataset\n",
    "    !pip install -q kaggle\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp kaggle.json ~/.kaggle/\n",
    "    !ls ~/.kaggle\n",
    "    !chmod 600 /root/.kaggle/kaggle.json\n",
    "    !kaggle datasets download moltean/fruits\n",
    "    !unzip -q fruits.zip\n",
    "    dataset_root = 'fruits-360'\n",
    "elif upsample_mode == 'gan':\n",
    "    # download the GAN upsampled dataset\n",
    "    !gdown --id 10Omg4-7u4yfAlQTRfkJm1DIB-bsZEPuh\n",
    "    !unzip -q fruits-360-gan\n",
    "    dataset_root = 'fruits-360-gan'\n",
    "else:\n",
    "    raise NotImplementedError(\"Unknown upsampling mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the dataset and loaders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'pro':\n",
    "    caption_templates = [\n",
    "        \"a photo of the small {}, type of fruit\", \n",
    "        \"a close-up photo of a {}, type of fruit\", \n",
    "        \"a cropped photo of the {}, type of fruit\"\n",
    "    ]\n",
    "\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    text_transformer = TextTransformer(\n",
    "        tokenizer, \n",
    "        templates=caption_templates,\n",
    "        context_length=context_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine transformations if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "if upsample_mode == 'bicubic':\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize(input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(input_resolution),\n",
    "        # augmentation\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomPerspective(),\n",
    "        T.RandomRotation(degrees=20, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        #\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(MEAN, STD)                              \n",
    "    ])\n",
    "\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize(input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(input_resolution),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(MEAN, STD)                              \n",
    "    ])\n",
    "\n",
    "else:\n",
    "    train_transform = T.Compose([\n",
    "        # augmentation\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomPerspective(),\n",
    "        T.RandomRotation(degrees=20, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        #\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(MEAN, STD)                              \n",
    "    ])\n",
    "\n",
    "    val_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(MEAN, STD)                              \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_suffix = 'Training' if upsample_mode == 'bicubic' else 'Train'\n",
    "\n",
    "train_val_dataset = CLIPDataset(\n",
    "    f'{dataset_root}/{train_suffix}', \n",
    "    image_transform=train_transform, \n",
    "    return_indices=True\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = train_val_split(train_val_dataset, val_size=0.1, random_state=42)\n",
    "train_dataset.image_transform = train_transform\n",
    "val_dataset.image_transform = val_transform\n",
    "\n",
    "test_dataset = CLIPDataset(\n",
    "    f'{dataset_root}/Test', \n",
    "    image_transform=val_transform, \n",
    "    return_indices=True\n",
    ")\n",
    "\n",
    "test_dataset.idx_to_class = train_dataset.idx_to_class\n",
    "test_dataset.class_to_idx = train_dataset.class_to_idx"
   ]
  },
  {
   "source": [
    "Init tensor of captions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'pro':\n",
    "    num_classes = len(train_dataset.class_to_idx)\n",
    "    num_captions = len(caption_templates)\n",
    "\n",
    "    tokenized_captions = torch.zeros((num_classes, num_captions, context_length), dtype=torch.int)\n",
    "\n",
    "    for idx, class_name in train_dataset.idx_to_class.items():\n",
    "        class_captions = text_transformer(class_name)\n",
    "        tokenized_captions[idx] = class_captions\n",
    "\n",
    "    tokenized_captions = tokenized_captions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count())\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=os.cpu_count())\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training procedure\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Pytorch Lightning modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "seed_everything(seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'lite':\n",
    "    # 1024 for RN50, 512 for ViT/RN101\n",
    "    clip_wrapper = CLIP_Lite(model, num_classes=131, clip_out_features=512)\n",
    "if clip_mode == 'pro':\n",
    "    clip_wrapper = CLIP_Pro(model, tokenized_captions, clip_out_features=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f'logs/CLIP_{clip_mode}_{clip_backbone}'\n",
    "logger = TensorBoardLogger(log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir, monitor='val/accuracy', mode='max')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    gradient_clip_val=1,\n",
    "    amp_backend='native',\n",
    "    deterministic=True,\n",
    "    auto_lr_find=True,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "trainer.tune(clip_wrapper, train_dataloader=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(clip_wrapper, train_loader, val_loader)\n",
    "clip_wrapper.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n",
    "trainer.test(clip_wrapper, test_loader)"
   ]
  },
  {
   "source": [
    "Tensorboard logger\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize predictions of the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clip_wrapper.parameters():\n",
    "  param = param.cuda()\n",
    "\n",
    "visualizer = ClassificationVisualizer(\n",
    "    clip_wrapper, \n",
    "    train_dataset, \n",
    "    images_in_row=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_predictions(num_images=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('base': conda)",
   "name": "python392jvsc74a57bd0c97ba66917de3b8304ab431763ba637115ffd393743cb38a2e6c2e42cca64418"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "c97ba66917de3b8304ab431763ba637115ffd393743cb38a2e6c2e42cca64418"
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}