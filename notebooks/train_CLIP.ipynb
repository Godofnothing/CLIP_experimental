{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Godofnothing/CLIP_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch-lightning\n",
    "!pip install -q ftfy regex\n",
    "!wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"CLIP_experimental\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src import CLIP_Lite, CLIP_Pro\n",
    "from src import TextTransformer\n",
    "from src import SimpleTokenizer\n",
    "from src import CLIPDataset, train_val_split\n",
    "from src import ClassificationVisualizer\n",
    "\n",
    "from srgan import GANUpsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "assert device == 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lite or pro\n",
    "clip_mode = 'lite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models available\n",
    "- RN50\n",
    "- RN101\n",
    "- RN50x4\n",
    "- ViT-B/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/openai/CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP import clip\n",
    "\n",
    "# set jit=False to case to nn.Module from torchscript \n",
    "model, image_transform = clip.load(\"ViT-B/32\", jit=False)\n",
    "input_resolution = 224\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset from kaggle\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download moltean/fruits\n",
    "!unzip -q fruits.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = 'fruits-360'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the dataset and loaders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'pro':\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    text_transformer = TextTransformer(tokenizer, context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine transformations if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bicubic or GAN upsample\n",
    "upsample_mode = 'bicubic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize(input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(input_resolution),\n",
    "    # augmentation\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomPerspective(),\n",
    "    T.RandomRotation(degrees=20, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "    #\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(MEAN, STD)                              \n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize(input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(input_resolution),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(MEAN, STD)                              \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = CLIPDataset(\n",
    "    f'{dataset_root}/Training', \n",
    "    return_indices=True\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = train_val_split(train_val_dataset, val_size=0.1, random_state=42)\n",
    "train_dataset.image_transform = train_transform\n",
    "val_dataset.image_transform = val_transform\n",
    "\n",
    "test_dataset = CLIPDataset(\n",
    "    f'{dataset_root}/Test', \n",
    "    image_transform=val_transform, \n",
    "    return_indices=True\n",
    ")"
   ]
  },
  {
   "source": [
    "Init tensor of captions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'pro':\n",
    "    num_classes = len(train_dataset.class_to_idx)\n",
    "    num_captions = 2\n",
    "\n",
    "    tokenized_captions = torch.zeros((num_classes, num_captions, context_length), dtype=torch.int)\n",
    "\n",
    "    for idx, class_name in train_dataset.idx_to_class.items():\n",
    "        class_captions = text_transformer(class_name)\n",
    "        tokenized_captions[idx] = class_captions\n",
    "\n",
    "    tokenized_captions = tokenized_captions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training procedure\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Pytorch Lightning modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'lite':\n",
    "    # 1024 for RN, 512 for ViT\n",
    "    clip_wrapper = CLIP_Lite(model, num_classes=131, clip_out_features=1024, training_mode=train_mode)\n",
    "if clip_mode == 'pro':\n",
    "    clip_wrapper = CLIP_Pro(model, training_mode=train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f'logs/CLIP_{clip_mode}'\n",
    "logger = TensorBoardLogger(log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir, mode='max')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    gradient_clip_val=1e-3,\n",
    "    amp_backend='native',\n",
    "    auto_lr_find=True,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "trainer.tune(clip_wrapper, train_dataloader=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(clip_wrapper, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize predictions of the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clip_wrapper.parameters():\n",
    "  param = param.cuda()\n",
    "\n",
    "visualizer = ClassificationVisualizer(\n",
    "    clip_wrapper, \n",
    "    train_dataset, \n",
    "    images_in_row=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_predictions(num_images=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('base': conda)",
   "name": "python392jvsc74a57bd0c97ba66917de3b8304ab431763ba637115ffd393743cb38a2e6c2e42cca64418"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "c97ba66917de3b8304ab431763ba637115ffd393743cb38a2e6c2e42cca64418"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}