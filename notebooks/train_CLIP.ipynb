{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0c97ba66917de3b8304ab431763ba637115ffd393743cb38a2e6c2e42cca64418",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "c97ba66917de3b8304ab431763ba637115ffd393743cb38a2e6c2e42cca64418"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Preparations\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Clone the repository"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Godofnothing/CLIP_experimental"
   ]
  },
  {
   "source": [
    "Installation of the dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch-lightning\n",
    "!pip install -q ftfy regex\n",
    "!wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"CLIP_experimental\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src import CLIP_Lite, CLIP_Pro\n",
    "from src import TextTransformer\n",
    "from src import SimpleTokenizer\n",
    "from src import CLIPDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lite or pro\n",
    "clip_mode = 'lite'\n",
    "# classic or cosine_similarity for lite | visual or visual&text for pro\n",
    "train_mode = 'classic'"
   ]
  },
  {
   "source": [
    "### Download the model\n",
    "___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Models available\n",
    "- RN50\n",
    "- RN101\n",
    "- RN50x4\n",
    "- ViT-B/32"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/openai/CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP import clip\n",
    "\n",
    "# set jit=False to case to nn.Module from torchscript \n",
    "model, image_transform = clip.load(\"ViT-B/32\", jit=False)\n",
    "input_resolution = 224\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "source": [
    "### Get dataset from kaggle\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download moltean/fruits\n",
    "!unzip -q fruits.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = 'fruits-360'"
   ]
  },
  {
   "source": [
    "### Construction of the dataset and loaders\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'pro':\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    text_transformer = TextTransformer(tokenizer, context_length)"
   ]
  },
  {
   "source": [
    "Redefine transformations if needed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "image_transform = T.Compose([\n",
    "    T.Resize(input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(input_resolution),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(MEAN, STD)                              \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'lite' and train_mode == 'classic':\n",
    "    train_dataset = CLIPDataset(\n",
    "        f'{dataset_root}/Training', \n",
    "        image_transform=image_transform, \n",
    "        return_indices=True\n",
    "    )\n",
    "\n",
    "    val_dataset = CLIPDataset(\n",
    "        f'{dataset_root}/Test', \n",
    "        image_transform=image_transform, \n",
    "        return_indices=True\n",
    "    )\n",
    "else:\n",
    "    train_dataset = CLIPDataset(\n",
    "        f'{dataset_root}/Training', \n",
    "        image_transform=image_transform, \n",
    "        prompt_transform=text_transformer,\n",
    "        return_indices=False\n",
    "    )\n",
    "\n",
    "    val_dataset = CLIPDataset(\n",
    "        f'{dataset_root}/Test', \n",
    "        image_transform=image_transform, \n",
    "        prompt_transform=text_transformer,\n",
    "        return_indices=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "source": [
    "### The training procedure\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Init Pytorch Lightning modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'lite':\n",
    "    # 1024 for RN, 512 for ViT\n",
    "    clip_wrapper = CLIP_Lite(model, num_classes=131, clip_out_features=1024, training_mode=train_mode)\n",
    "if clip_mode == 'pro':\n",
    "    clip_wrapper = CLIP_Pro(model, training_mode=train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    gradient_clip_val=1e-3,\n",
    "    amp_backend='native',\n",
    "    auto_lr_find=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(clip_wrapper, train_loader, val_loader)"
   ]
  }
 ]
}