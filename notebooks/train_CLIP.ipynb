{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0c97ba66917de3b8304ab431763ba637115ffd393743cb38a2e6c2e42cca64418",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "c97ba66917de3b8304ab431763ba637115ffd393743cb38a2e6c2e42cca64418"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from src import CLIP_Lite, CLIP_Pro\n",
    "from src import TextTransformer\n",
    "from src import SimpleTokenizer\n",
    "from src import CLIPDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lite or pro\n",
    "clip_mode = 'lite'\n",
    "\n",
    "train_mode = 'classic'"
   ]
  },
  {
   "source": [
    "### Download the model\n",
    "___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "List all available models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
    "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
    "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget {MODELS[\"RN50\"]} -O model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(\"model.pt\").to(device)\n",
    "input_resolution = model.input_resolution.item()\n",
    "context_length = model.context_length.item()\n",
    "vocab_size = model.vocab_size.item()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "source": [
    "### Get dataset from kaggle\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download moltean/fruits\n",
    "!unzip -q fruits.zip"
   ]
  },
  {
   "source": [
    "Install regexp packages for tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ftfy regex\n",
    "!wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
   ]
  },
  {
   "source": [
    "### Construction of the dataset and loaders\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'pro':\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    text_transformer = TextTransformer(tokenizer, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "image_transform = T.Compose([\n",
    "    T.Resize(input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(input_resolution),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(MEAN, STD)                              \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'lite' and train_mode == 'classic':\n",
    "    train_dataset = CLIPDataset(\n",
    "        f'{dataset_root}/Training', \n",
    "        image_transform=image_transform, \n",
    "        return_indices=True\n",
    "    )\n",
    "\n",
    "    val_dataset = CLIPDataset(\n",
    "        f'{dataset_root}/Test', \n",
    "        image_transform=image_transform, \n",
    "        return_indices=True\n",
    "    )\n",
    "else:\n",
    "    train_dataset = CLIPDataset(\n",
    "        f'{dataset_root}/Training', \n",
    "        image_transform=image_transform, \n",
    "        prompt_transform=text_transformer,\n",
    "        return_indices=False\n",
    "    )\n",
    "\n",
    "    val_dataset = CLIPDataset(\n",
    "        f'{dataset_root}/Test', \n",
    "        image_transform=image_transform, \n",
    "        prompt_transform=text_transformer,\n",
    "        return_indices=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "source": [
    "### The training procedure\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Init Pytorch Lightning modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_mode == 'lite':\n",
    "    clip_lite = CLIP_Lite(model, num_classes=131, training_mode=train_mode)\n",
    "if clip_mode == 'pro':\n",
    "    clip_lite = CLIP_Pro(model, num_classes=131, training_mode=train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    gradient_clip_val=1e-3,\n",
    "    amp_backend='native',\n",
    "    auto_lr_find=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(clip_lite, train_loader, val_loader)"
   ]
  }
 ]
}